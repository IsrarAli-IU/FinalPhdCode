# -*- coding: utf-8 -*-
"""Relation_Aware_Bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-954669oS56NI7phMmtyBKGQgd2BUyPz
"""

import torch
import torch.nn as nn
from transformers import BertTokenizer, BertModel
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, precision_recall_curve, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Define Relation-Aware BERT (RABERT) Model
class RelationAwareBERT(nn.Module):
    def __init__(self, pretrained_model='bert-base-uncased', relation_dim=16, num_labels=2):
        super(RelationAwareBERT, self).__init__()

        # BERT backbone
        self.bert = BertModel.from_pretrained(pretrained_model)

        # Relation embeddings
        self.relation_embedding = nn.Embedding(100, relation_dim)  # Assume max 100 unique relations

        # Classification head
        self.fc = nn.Linear(self.bert.config.hidden_size + relation_dim, num_labels)

    def forward(self, input_ids, attention_mask, relation_ids):
        # Standard BERT forward pass
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token representation

        # Incorporate relational embeddings
        relation_embeds = self.relation_embedding(relation_ids)  # Shape: (batch_size, relation_dim)
        cls_embedding = torch.cat([cls_embedding, relation_embeds], dim=1)  # Concatenate

        # Classification
        logits = self.fc(cls_embedding)
        return logits

# Custom Dataset Class
class RelationAwareDataset(Dataset):
    def __init__(self, data, tokenizer, max_length, relation_map):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.relation_map = relation_map

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        row = self.data.iloc[index]
        features = row.drop('LargeClass').to_dict()
        text = ', '.join([f"{key}: {value}" for key, value in features.items()])
        label = row['LargeClass']

        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        # Generate relation IDs (this is a placeholder; customize based on relationships in your data)
        relation_id = self.relation_map.get('default', 0)

        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'relation_ids': torch.tensor(relation_id, dtype=torch.long),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Load Dataset
data_path = 'lcd.csv'
data_df = pd.read_csv(data_path)

# Define relationships (customize as per dataset specifics)
relation_map = {
    'default': 0  # Placeholder for relation IDs
}

# Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Train-Test Split
train_data, test_data = train_test_split(data_df, test_size=0.2, random_state=42, stratify=data_df['LargeClass'])
train_dataset = RelationAwareDataset(train_data, tokenizer, max_length=128, relation_map=relation_map)
test_dataset = RelationAwareDataset(test_data, tokenizer, max_length=128, relation_map=relation_map)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)

# Initialize Model
model = RelationAwareBERT(pretrained_model='bert-base-uncased', relation_dim=16, num_labels=2)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = model.to(device)

# Optimizer and Loss Function
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
criterion = nn.CrossEntropyLoss()

# Training Function
def train_model():
    model.train()
    total_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        relation_ids = batch['relation_ids'].to(device)
        labels = batch['labels'].to(device)

        logits = model(input_ids, attention_mask, relation_ids)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    return total_loss / len(train_loader)

# Evaluation Function
def evaluate_model():
    model.eval()
    preds, true_labels, probs = [], [], []
    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            relation_ids = batch['relation_ids'].to(device)
            labels = batch['labels'].to(device)

            logits = model(input_ids, attention_mask, relation_ids)
            probabilities = torch.softmax(logits, dim=1)[:, 1]

            probs.extend(probabilities.cpu().numpy())
            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
            true_labels.extend(labels.cpu().numpy())

    # Classification Report
    report = classification_report(true_labels, preds, target_names=['Not LargeClass', 'LargeClass'])
    print("\nClassification Report:\n", report)

    # Confusion Matrix
    cm = confusion_matrix(true_labels, preds)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not LargeClass', 'LargeClass'], yticklabels=['Not LargeClass', 'LargeClass'])
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

    # Precision-Recall Curve
    precision, recall, _ = precision_recall_curve(true_labels, probs)
    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, marker='.', label='Precision-Recall Curve')
    plt.title('Precision-Recall Curve')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.legend()
    plt.show()

# Training Loop
for epoch in range(3):
    loss = train_model()
    print(f"Epoch {epoch + 1}, Loss: {loss:.4f}")

evaluate_model()